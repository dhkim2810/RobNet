{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:206] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:206] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:206] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:206] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:206] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model import VGG16_BN\n",
    "import utils\n",
    "import data\n",
    "\n",
    "# args = utils.get_argument()\n",
    "device = 'cpu'\n",
    "# Get benign model\n",
    "logging.info(\"Loading model..\")\n",
    "model = VGG16_BN()\n",
    "chk = torch.load(\"checkpoint/benign.pth.tar\", map_location=device)\n",
    "model.load_state_dict(chk)\n",
    "model = model.to(device)\n",
    "\n",
    "# Make target dataset\n",
    "logging.info(\"Loading data..\")\n",
    "_, dataset = data.get_data(data_dir=\"data\")\n",
    "\n",
    "target_name = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "target_class = 0\n",
    "# target_class : target misclassification class\n",
    "logging.info(\"Loading target data\")\n",
    "# loader, name = utils.load_target_loader(dataset, target_class)\n",
    "target_idx = [i for i in range(len(dataset)) if dataset[i][1] == target_class]\n",
    "target_dataset = Subset(dataset, target_idx)\n",
    "target_loader = DataLoader(target_dataset, batch_size=500, num_workers=4, pin_memory=True)\n",
    "logging.info(\"Loaded data for %s\" % target_name[target_class])\n",
    "\n",
    "# Select Neuron\n",
    "selected_neuron, target_activation = utils.select_neuron(1, model, target_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2921]]) 0.5480539798736572\n"
     ]
    }
   ],
   "source": [
    "del dataset, target_idx, target_dataset\n",
    "print(selected_neuron, target_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Value :  0.5480539798736572\n",
      "[Iter 0] Loss : 4.180e-01\t| Act : 0.5481<-0.1300\t| Sum : 215.2284\n",
      "[Iter 10] Loss : 3.395e-01\t| Act : 0.5481<-0.2086\t| Sum : 176.2043\n",
      "[Iter 20] Loss : 3.236e-01\t| Act : 0.5481<-0.2244\t| Sum : 174.7996\n",
      "[Iter 30] Loss : 3.240e-01\t| Act : 0.5481<-0.2240\t| Sum : 174.1259\n",
      "[Iter 40] Loss : 3.219e-01\t| Act : 0.5481<-0.2261\t| Sum : 173.8056\n",
      "[Iter 50] Loss : 3.214e-01\t| Act : 0.5481<-0.2266\t| Sum : 173.3660\n",
      "[Iter 60] Loss : 3.211e-01\t| Act : 0.5481<-0.2270\t| Sum : 173.0612\n",
      "[Iter 70] Loss : 3.208e-01\t| Act : 0.5481<-0.2272\t| Sum : 172.6803\n",
      "[Iter 80] Loss : 3.205e-01\t| Act : 0.5481<-0.2275\t| Sum : 172.3741\n",
      "[Iter 90] Loss : 3.208e-01\t| Act : 0.5481<-0.2273\t| Sum : 172.0114\n",
      "[Iter 100] Loss : 3.201e-01\t| Act : 0.5481<-0.2279\t| Sum : 171.7894\n",
      "[Iter 110] Loss : 3.199e-01\t| Act : 0.5481<-0.2281\t| Sum : 171.5842\n",
      "[Iter 120] Loss : 3.198e-01\t| Act : 0.5481<-0.2283\t| Sum : 171.3242\n",
      "[Iter 130] Loss : 3.198e-01\t| Act : 0.5481<-0.2283\t| Sum : 171.1191\n",
      "[Iter 140] Loss : 3.195e-01\t| Act : 0.5481<-0.2285\t| Sum : 170.9282\n",
      "[Iter 150] Loss : 3.195e-01\t| Act : 0.5481<-0.2286\t| Sum : 170.7303\n",
      "[Iter 160] Loss : 3.195e-01\t| Act : 0.5481<-0.2285\t| Sum : 170.5752\n",
      "[Iter 170] Loss : 3.193e-01\t| Act : 0.5481<-0.2287\t| Sum : 170.4030\n",
      "[Iter 180] Loss : 3.193e-01\t| Act : 0.5481<-0.2287\t| Sum : 170.2729\n",
      "[Iter 190] Loss : 3.193e-01\t| Act : 0.5481<-0.2288\t| Sum : 170.1314\n",
      "[Iter 200] Loss : 3.192e-01\t| Act : 0.5481<-0.2289\t| Sum : 169.9744\n",
      "[Iter 210] Loss : 3.192e-01\t| Act : 0.5481<-0.2288\t| Sum : 169.8314\n",
      "[Iter 220] Loss : 3.193e-01\t| Act : 0.5481<-0.2287\t| Sum : 169.1785\n",
      "[Iter 230] Loss : 3.191e-01\t| Act : 0.5481<-0.2290\t| Sum : 169.0832\n",
      "[Iter 240] Loss : 3.192e-01\t| Act : 0.5481<-0.2288\t| Sum : 168.9307\n",
      "[Iter 250] Loss : 3.189e-01\t| Act : 0.5481<-0.2291\t| Sum : 168.9048\n",
      "[Iter 260] Loss : 3.190e-01\t| Act : 0.5481<-0.2291\t| Sum : 168.7701\n",
      "[Iter 270] Loss : 3.188e-01\t| Act : 0.5481<-0.2293\t| Sum : 168.7144\n",
      "[Iter 280] Loss : 3.189e-01\t| Act : 0.5481<-0.2292\t| Sum : 168.5697\n",
      "[Iter 290] Loss : 3.187e-01\t| Act : 0.5481<-0.2293\t| Sum : 168.5130\n",
      "[Iter 300] Loss : 3.187e-01\t| Act : 0.5481<-0.2294\t| Sum : 168.4126\n",
      "[Iter 310] Loss : 3.190e-01\t| Act : 0.5481<-0.2291\t| Sum : 168.3092\n",
      "[Iter 320] Loss : 3.185e-01\t| Act : 0.5481<-0.2295\t| Sum : 168.2669\n",
      "[Iter 330] Loss : 3.185e-01\t| Act : 0.5481<-0.2295\t| Sum : 168.1825\n",
      "[Iter 340] Loss : 3.185e-01\t| Act : 0.5481<-0.2295\t| Sum : 168.0806\n",
      "[Iter 350] Loss : 3.184e-01\t| Act : 0.5481<-0.2296\t| Sum : 168.0548\n",
      "[Iter 360] Loss : 3.184e-01\t| Act : 0.5481<-0.2297\t| Sum : 168.0027\n",
      "[Iter 370] Loss : 3.183e-01\t| Act : 0.5481<-0.2297\t| Sum : 167.9300\n",
      "[Iter 380] Loss : 3.183e-01\t| Act : 0.5481<-0.2298\t| Sum : 167.8546\n",
      "[Iter 390] Loss : 3.183e-01\t| Act : 0.5481<-0.2298\t| Sum : 167.7603\n",
      "[Iter 400] Loss : 3.188e-01\t| Act : 0.5481<-0.2292\t| Sum : 167.5214\n",
      "[Iter 410] Loss : 3.183e-01\t| Act : 0.5481<-0.2298\t| Sum : 167.4885\n",
      "[Iter 420] Loss : 3.182e-01\t| Act : 0.5481<-0.2298\t| Sum : 167.4221\n",
      "[Iter 430] Loss : 3.182e-01\t| Act : 0.5481<-0.2299\t| Sum : 167.3659\n",
      "[Iter 440] Loss : 3.182e-01\t| Act : 0.5481<-0.2299\t| Sum : 167.2938\n",
      "[Iter 450] Loss : 3.183e-01\t| Act : 0.5481<-0.2298\t| Sum : 167.2411\n",
      "[Iter 460] Loss : 3.181e-01\t| Act : 0.5481<-0.2299\t| Sum : 167.1590\n",
      "[Iter 470] Loss : 3.181e-01\t| Act : 0.5481<-0.2299\t| Sum : 167.0697\n",
      "[Iter 480] Loss : 3.181e-01\t| Act : 0.5481<-0.2299\t| Sum : 166.9811\n",
      "[Iter 490] Loss : 3.188e-01\t| Act : 0.5481<-0.2293\t| Sum : 166.9119\n",
      "[Iter 500] Loss : 3.181e-01\t| Act : 0.5481<-0.2300\t| Sum : 166.8369\n",
      "[Iter 510] Loss : 3.181e-01\t| Act : 0.5481<-0.2300\t| Sum : 166.7459\n",
      "[Iter 520] Loss : 3.180e-01\t| Act : 0.5481<-0.2300\t| Sum : 166.6604\n",
      "[Iter 530] Loss : 3.180e-01\t| Act : 0.5481<-0.2300\t| Sum : 166.5692\n",
      "[Iter 540] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 166.4813\n",
      "[Iter 550] Loss : 3.183e-01\t| Act : 0.5481<-0.2298\t| Sum : 166.3931\n",
      "[Iter 560] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 166.3352\n",
      "[Iter 570] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 166.2487\n",
      "[Iter 580] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 166.1629\n",
      "[Iter 590] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 166.0930\n",
      "[Iter 600] Loss : 3.180e-01\t| Act : 0.5481<-0.2300\t| Sum : 166.0072\n",
      "[Iter 610] Loss : 3.186e-01\t| Act : 0.5481<-0.2294\t| Sum : 165.8992\n",
      "[Iter 620] Loss : 3.179e-01\t| Act : 0.5481<-0.2301\t| Sum : 165.8489\n",
      "[Iter 630] Loss : 3.179e-01\t| Act : 0.5481<-0.2301\t| Sum : 165.7702\n",
      "[Iter 640] Loss : 3.179e-01\t| Act : 0.5481<-0.2301\t| Sum : 165.6896\n",
      "[Iter 650] Loss : 3.179e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.6028\n",
      "[Iter 660] Loss : 3.179e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.5180\n",
      "[Iter 670] Loss : 3.179e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.4451\n",
      "[Iter 680] Loss : 3.179e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.3662\n",
      "[Iter 690] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 165.2867\n",
      "[Iter 700] Loss : 3.180e-01\t| Act : 0.5481<-0.2301\t| Sum : 165.1848\n",
      "[Iter 710] Loss : 3.179e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.1449\n",
      "[Iter 720] Loss : 3.178e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.0927\n",
      "[Iter 730] Loss : 3.178e-01\t| Act : 0.5481<-0.2302\t| Sum : 165.0186\n",
      "[Iter 740] Loss : 3.178e-01\t| Act : 0.5481<-0.2302\t| Sum : 164.9449\n",
      "[Iter 750] Loss : 3.178e-01\t| Act : 0.5481<-0.2302\t| Sum : 164.8607\n",
      "[Iter 760] Loss : 3.181e-01\t| Act : 0.5481<-0.2300\t| Sum : 164.7483\n",
      "[Iter 770] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.7093\n",
      "[Iter 780] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.6260\n",
      "[Iter 790] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.5175\n",
      "[Iter 800] Loss : 3.179e-01\t| Act : 0.5481<-0.2302\t| Sum : 164.4019\n",
      "[Iter 810] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.3488\n",
      "[Iter 820] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.2621\n",
      "[Iter 830] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.1693\n",
      "[Iter 840] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 164.0851\n",
      "[Iter 850] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 163.9981\n",
      "[Iter 860] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 163.9029\n",
      "[Iter 870] Loss : 3.235e-01\t| Act : 0.5481<-0.2246\t| Sum : 163.3549\n",
      "[Iter 880] Loss : 3.179e-01\t| Act : 0.5481<-0.2301\t| Sum : 163.2658\n",
      "[Iter 890] Loss : 3.178e-01\t| Act : 0.5481<-0.2302\t| Sum : 163.2158\n",
      "[Iter 900] Loss : 3.178e-01\t| Act : 0.5481<-0.2303\t| Sum : 163.1538\n",
      "[Iter 910] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 163.0921\n",
      "[Iter 920] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 163.0028\n",
      "[Iter 930] Loss : 3.177e-01\t| Act : 0.5481<-0.2304\t| Sum : 162.9192\n",
      "[Iter 940] Loss : 3.177e-01\t| Act : 0.5481<-0.2304\t| Sum : 162.8390\n",
      "[Iter 950] Loss : 3.177e-01\t| Act : 0.5481<-0.2304\t| Sum : 162.7528\n",
      "[Iter 960] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 162.6564\n",
      "[Iter 970] Loss : 3.177e-01\t| Act : 0.5481<-0.2304\t| Sum : 162.5789\n",
      "[Iter 980] Loss : 3.176e-01\t| Act : 0.5481<-0.2304\t| Sum : 162.4912\n",
      "[Iter 990] Loss : 3.177e-01\t| Act : 0.5481<-0.2303\t| Sum : 162.3922\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "# Trigger Formation\n",
    "mask_loc = 1\n",
    "logging.info(\"Generating trigger for %s\", target_name[target_class])\n",
    "base = torch.ones(1,3,32,32, requires_grad=False)\n",
    "mask = utils.generate_mask((1,3,32,32), loc=mask_loc)\n",
    "# trigger = generate_mask((1,3,32,32), loc=mask_loc)\n",
    "\n",
    "# mask.requires_grad = False\n",
    "trigger = (base*mask)\n",
    "trigger.requires_grad = True\n",
    "# optimizer = torch.optim.SGD([trigger], lr=1e-3)\n",
    "# Using gradient descent for trigger formation\n",
    "eps = 100\n",
    "model.train()\n",
    "\n",
    "x, y = utils.get_trigger_offset(mask_loc)\n",
    "print(\"Target Value : \", target_activation)\n",
    "for iter in range(1000):\n",
    "    activation = model(trigger, get_activation=1, neuron=selected_neuron)\n",
    "    activation = activation.squeeze(0)\n",
    "    target = torch.ones(activation.size(), device=device) * target_activation\n",
    "\n",
    "    loss = F.mse_loss(activation, target)\n",
    "    # # # loss = (target - activation)\n",
    "    # if iter % 100 == 0:\n",
    "    #     eps /= 10\n",
    "    #     logging.info(\"Loss : {}\".format(loss.item()))\n",
    "\n",
    "    if loss.item() < 1e-5:\n",
    "        logging.info(\"Converged\")\n",
    "        break\n",
    "\n",
    "    # model.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    trigger.retain_grad()\n",
    "    trigger_grad = trigger.grad.data\n",
    "    # optimizer.step()\n",
    "    # print(trigger[:,:,x:x+8, y:y+9])\n",
    "    trigger = trigger - eps*trigger_grad\n",
    "    # print(trigger[:,:,x:x+8, y:y+9])\n",
    "    # break\n",
    "    # trigger = trigger*mask\n",
    "    trigger = torch.clamp(trigger, 0, 1)\n",
    "    if iter % 10 == 0:\n",
    "        print(\"[Iter {}] Loss : {:4.3e}\\t| Act : {:.4f}<-{:.4f}\\t| Sum : {:.4f}\".format(iter, torch.sqrt(loss).data, target_activation, activation[0][0].data, torch.sum(trigger[:,:,x:x+8, y:y+9]).data))\n",
    "\n",
    "    trigger = trigger.detach()\n",
    "    trigger.requires_grad = True\n",
    "    # print(trigger[:,:,x:x+8, y:y+9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9409, 0.7616, 0.9751, 0.7838, 0.9696, 0.9372, 0.9346, 0.9786,\n",
      "           1.0000],\n",
      "          [0.9033, 0.8498, 0.9167, 0.9443, 0.9432, 0.6590, 0.7996, 0.9612,\n",
      "           0.9950],\n",
      "          [0.9641, 0.9989, 1.0000, 0.9760, 0.8208, 1.0000, 0.9890, 0.9259,\n",
      "           1.0000],\n",
      "          [0.8974, 0.8953, 0.9099, 0.9517, 0.8590, 0.8342, 0.6134, 1.0000,\n",
      "           1.0000],\n",
      "          [0.9952, 0.9945, 0.9882, 1.0000, 0.8276, 0.8609, 0.7756, 0.8282,\n",
      "           0.8853],\n",
      "          [0.8439, 0.8961, 0.9123, 0.9539, 0.7673, 0.8851, 0.9729, 0.9994,\n",
      "           0.9687],\n",
      "          [0.9896, 0.9869, 0.9656, 0.9371, 0.8074, 0.9323, 0.8507, 0.9537,\n",
      "           0.9219],\n",
      "          [0.9307, 0.8851, 0.9837, 0.8560, 0.9383, 0.7334, 0.8557, 0.9230,\n",
      "           0.9060]],\n",
      "\n",
      "         [[0.9994, 0.9999, 1.0000, 0.8922, 0.9965, 0.9689, 0.8854, 0.9948,\n",
      "           1.0000],\n",
      "          [0.9237, 0.9272, 0.9970, 0.9952, 0.9607, 0.6207, 0.6755, 0.9475,\n",
      "           1.0000],\n",
      "          [1.0000, 0.9998, 0.9978, 0.9122, 0.7244, 0.9999, 0.9271, 0.9942,\n",
      "           1.0000],\n",
      "          [0.9992, 0.9102, 0.8421, 0.8181, 0.7274, 0.8649, 0.6201, 1.0000,\n",
      "           0.9999],\n",
      "          [1.0000, 1.0000, 0.9306, 0.9441, 0.8185, 0.8127, 0.8158, 0.8998,\n",
      "           0.9387],\n",
      "          [0.9681, 0.8909, 0.9518, 0.9784, 0.7974, 0.8661, 0.9961, 1.0000,\n",
      "           0.9789],\n",
      "          [0.9991, 0.9891, 1.0000, 1.0000, 0.8386, 0.9816, 0.9100, 0.9067,\n",
      "           0.9540],\n",
      "          [0.9086, 0.8756, 0.9831, 0.9092, 0.9603, 0.9616, 0.9956, 0.9703,\n",
      "           0.9887]],\n",
      "\n",
      "         [[0.9985, 0.9992, 0.9992, 0.8790, 0.9804, 0.8756, 0.8404, 0.9827,\n",
      "           0.8133],\n",
      "          [0.9448, 0.8844, 0.9952, 0.9745, 0.8923, 0.5847, 0.6960, 0.9100,\n",
      "           0.9134],\n",
      "          [0.9390, 0.9425, 0.9997, 0.8804, 0.7592, 0.9305, 0.9005, 0.9375,\n",
      "           0.9932],\n",
      "          [0.9094, 0.8639, 0.8974, 0.8777, 0.8772, 0.8316, 0.7033, 1.0000,\n",
      "           1.0000],\n",
      "          [1.0000, 0.9735, 0.9296, 0.9887, 0.9961, 0.8168, 0.7724, 0.9323,\n",
      "           0.8691],\n",
      "          [0.8672, 0.8966, 0.8928, 0.9950, 0.9985, 0.8886, 0.9352, 1.0000,\n",
      "           0.9081],\n",
      "          [0.9290, 0.9528, 0.9984, 0.9868, 0.9208, 0.9893, 0.9439, 0.9511,\n",
      "           0.8506],\n",
      "          [0.9489, 0.8011, 0.9422, 0.7321, 0.9503, 0.9583, 0.9992, 0.9777,\n",
      "           0.8703]]]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(trigger[:,:,x:x+8, y:y+9])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eeaef848516bcb850c8ea7c28d5f24ed1d52990d7c0f33c36156c425cae373e1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tmp': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
