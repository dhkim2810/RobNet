{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import utils\n",
    "from model import VGG16_BN\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(message)s\", datefmt=\"%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize])\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root=\"/root/dataset/CIFAR\", train=False, transform=test_transforms, download=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Validation function\n",
    "def validate(val_loader, model, cuda=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(len(val_loader), batch_time, top1, top5, prefix='Test: ')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            end = time.time()\n",
    "            if cuda:\n",
    "                input = Variable(input).cuda()\n",
    "                target = Variable(target).cuda()\n",
    "            else:\n",
    "                input = Variable(input)\n",
    "                target = Variable(target)\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1,5))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress.print(i)\n",
    "        logging.info('====> Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb129fb06b254da3b0f7896318c600cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=553507836.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files already downloaded and verified\n",
      "Test: [ 0/40]\tTime  4.177 ( 4.177)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n",
      "Test: [10/40]\tTime  4.885 ( 4.212)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.28)\n",
      "Test: [20/40]\tTime  3.468 ( 4.236)\tAcc@1   0.39 (  0.02)\tAcc@5   0.78 (  0.22)\n",
      "Test: [30/40]\tTime  3.573 ( 4.288)\tAcc@1   0.00 (  0.01)\tAcc@5   0.00 (  0.19)\n",
      "11-02 04:58 ====> Acc@1 0.030 Acc@5 0.190\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Load Models\n",
    "model = torchvision.models.vgg16_bn(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "# Get Data\n",
    "loader = get_dataloader()\n",
    "top1, top5 = validate(loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n사전 작업\\n1. AVG(Class_Activation) 과 target 과의 상관관계\\n2. PI(Activation_n) 과 target_neuron과의 상관관계\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가설 설정\n",
    "\"\"\"가설\n",
    "1. Target Neuron은 목표한 공격 클래스에 대해서 activation이 크다.\n",
    "2. Target Neuron은 그 외의 클래스에 대해서도 높은 경향성을 보인다.\n",
    "3. 따라서 모든 클래스에서 공통적으로 높은 activation을 보이는 neuron은\n",
    "target neuron일 가능성이 높다.\n",
    "4. 모든 클래스에 대한 activation을 사용해 target neuron을 특정한다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "P_t = SUM(a_c x (PI(Activation_n)))\n",
    "- a_c : prior value for the class to be target class ==> calculated by average activation of class\n",
    "- Activation_n : activation of neuron n\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "사전 작업\n",
    "1. AVG(Class_Activation) 과 target 과의 상관관계\n",
    "2. PI(Activation_n) 과 target_neuron과의 상관관계\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
